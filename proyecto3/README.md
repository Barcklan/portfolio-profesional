# ğŸ” Proyecto 3 â€” Interpretabilidad de Modelos Predictivos usando LIME y SHAP

> ### Explicabilidad y Ã©tica en inteligencia artificial

---

## ğŸ¯ Objetivo
Aplicar herramientas de **explicabilidad de modelos**, como **LIME** y **SHAP**, para analizar y justificar el comportamiento de un modelo de clasificaciÃ³n, destacando la importancia de la **transparencia y Ã©tica** en IA.

---

## ğŸ§© Contexto
Formar parte de un equipo que usa **IA en decisiones crÃ­ticas** implica construir modelos **explicables y auditables**.  
Este proyecto muestra cÃ³mo la interpretabilidad permite **detectar errores y sesgos ocultos** y mejorar la confianza en las decisiones automatizadas.

---

## ğŸ“Š Resumen del Proyecto
Se analizÃ³ un modelo de **Random Forest** para predecir **enfermedades cardÃ­acas**, utilizando **LIME** y **SHAP**:

- PrecisiÃ³n general: **88.6%**  
- Uso correcto de variables relevantes: **ECG, angina, frecuencia cardÃ­aca mÃ¡xima**  
- Problema detectado: valores de **colesterol mal interpretados** (`0.0`), causando falsos positivos

ğŸ“ˆ El proyecto demuestra que **precisiÃ³n sin interpretabilidad** no es suficiente, y que la transparencia es esencial en IA crÃ­tica.

---

## ğŸ§° TecnologÃ­as Utilizadas
- Python  
- Scikit-learn  
- Random Forest  
- LIME  
- SHAP  
- Matplotlib / Seaborn  
- Pandas / NumPy

---

## ğŸ“‚ Estructura de Archivos

```bash
 proyecto3/               
   â”œâ”€ ğŸ“œ README.md              â† Carpeta del Proyecto 3
   â”œâ”€ ğŸ“‚ data/                 
   â”œâ”€ ğŸ“” notebooks/             
   â”œâ”€ ğŸ“‚ scripts/               
   â”œâ”€ ğŸ“‚ reports/              
   â””â”€ ğŸ“œ requirements.txt       
  ```
--- 

## ğŸ“ˆ Resultados Principales/Hallazgos

El anÃ¡lisis de un modelo de Random Forest para predecir enfermedades cardÃ­acas utilizando herramientas de interpretabilidad como SHAP y LIME ha revelado que, aunque el modelo tiene una alta precisiÃ³n general (88.6%), su lÃ³gica interna es defectuosa y potencialmente peligrosa. La interpretabilidad demostrÃ³ que el modelo se apoya en variables clÃ­nicamente vÃ¡lidas y de peso, como los patrones de ECG durante el ejercicio, la angina y la frecuencia cardÃ­aca mÃ¡xima. Sin embargo, tambiÃ©n se descubriÃ³ una falla crÃ­tica: el modelo interpreta de manera inconsistente y anÃ³mala la variable Colesterol, tratando los valores bajos como un factor de riesgo significativo y los altos como protectores.

Esta inconsistencia, probablemente causada por errores de entrada de datos (valores de 0.0), llevÃ³ a predicciones incorrectas en casos especÃ­ficos, generando falsos positivos a pesar de que el paciente no presentaba una enfermedad cardÃ­aca. Este proyecto subraya que la precisiÃ³n no es suficiente en Ã¡reas crÃ­ticas como la salud. Sin interpretabilidad, no se podrÃ­a auditar la lÃ³gica del modelo, identificar sus sesgos y vulnerabilidades, o comprender por quÃ© falla en casos particulares, lo que demuestra que la transparencia y la responsabilidad son esenciales para la implementaciÃ³n segura y Ã©tica de la inteligencia artificial.

A futuro, se planearÃ¡ una mejora del modelo de Random Forest, enfocada en una depuraciÃ³n y validaciÃ³n exhaustiva de los datos de entrada, asÃ­ como en la optimizaciÃ³n de los hiperparÃ¡metros y la evaluaciÃ³n comparativa con otros algoritmos como XGBoost o modelos basados en redes neuronales. AdemÃ¡s, se buscarÃ¡ incorporar un proceso continuo de monitorizaciÃ³n del rendimiento y reentrenamiento con nuevos datos clÃ­nicos, garantizando asÃ­ un modelo mÃ¡s robusto, confiable y alineado con los principios de la medicina basada en evidencia.

<p align="center">
  <img src="img/Metricas.png" width="45.7%" />
  <img src="img/F1-Score.png" width="45%" />
</p>

<div align="center">

 <H3> Modelo Random Forest entrenado </H3>
  <H4> MÃ©tricas en prueba: <b>accuracy: 0.89</b> </H4>
|Estado CardÃ­aco Paciente| `precision` | `recall` | `F1-Score` |
|-----------|-----------|-----------|-----------|
|Sano (=0)| 0.89 | 0.85 | 0.87 |
|Enfermo(=1)| 0.89 | 0.91 | 0.90 |


</div>



Tanto Naive Bayes como BERT alcanzaron un rendimiento perfecto (100% en precisiÃ³n, recall, F1 y accuracy), lo que sugiere que el dataset es pequeÃ±o y fÃ¡cilmente separable, con posible sobreajuste. No se observa ventaja entre ambos modelos: Naive Bayes es mÃ¡s rÃ¡pido y eficiente para tareas simples, mientras que BERT ofrece mayor robustez para escenarios mÃ¡s complejos o con mayor volumen de datos.

### Explicabilidad con LIME:

Dado que ambos modelos â€”Naive Bayes y BERTâ€” alcanzaron un rendimiento perfecto (1.00 en accuracy, precision, recall y F1-score), resulta fundamental analizar cÃ³mo y por quÃ© llegan a sus predicciones. La interpretabilidad mediante LIME (Local Interpretable Model-agnostic Explanations) permite comprender quÃ© palabras o patrones lingÃ¼Ã­sticos influyen mÃ¡s en la clasificaciÃ³n de la gravedad clÃ­nica del paciente (leve, moderado o severo).

A travÃ©s de LIME, se busca verificar si las decisiones de los modelos son coherentes con el contexto mÃ©dico, identificar posibles errores de interpretaciÃ³n semÃ¡ntica y garantizar que la alta precisiÃ³n observada no oculte sesgos o sobreajuste hacia ciertas clases o tÃ©rminos clÃ­nicos.

<div align="center">
  <H3> Naive Bayes </H3>
</div>
<p align="center">
  <img src="img/LIME_BN.png" width="60%" />
</p>

El modelo Naive Bayes clasificÃ³ el texto como â€œleveâ€ con un 98% de probabilidad. LIME evidenciÃ³ que palabras como complicaciones o severas, aunque sugieren mayor gravedad, fueron interpretadas por el modelo como asociadas a casos leves, descartando asÃ­ las clases â€œmoderadoâ€ y â€œseveroâ€.

<div align="center">
  <H3>BERT</H3> 
</div>
<p align="center">
  <img src="img/LIME_BERT.png" width="80%" />
</p>

El modelo BERT clasificÃ³ el texto como â€œseveroâ€ (52%) al identificar tÃ©rminos clave como dificultad respiratoria, hospitalizaciÃ³n y requiere inmediata, asociados a alta gravedad. Aunque las clases â€œleveâ€ y â€œmoderadoâ€ tuvieron cierta probabilidad, las palabras clÃ­nicas reforzaron la decisiÃ³n hacia â€œseveroâ€.


## ğŸ“„ Conclusiones

El proyecto demostrÃ³ que las tÃ©cnicas de NLP pueden clasificar eficazmente notas clÃ­nicas segÃºn la gravedad del paciente. Naive Bayes ofreciÃ³ una lÃ­nea base interpretable, mientras que BERT logrÃ³ mayor comprensiÃ³n semÃ¡ntica y precisiÃ³n. El uso de LIME aportÃ³ transparencia al mostrar las palabras clave que influyen en las predicciones. AdemÃ¡s, se destacÃ³ la necesidad de abordar sesgos, privacidad y supervisiÃ³n mÃ©dica, promoviendo un uso Ã©tico y responsable de la IA como herramienta de apoyo en la detecciÃ³n y priorizaciÃ³n clÃ­nica.


#### ğŸ”— [Ver anÃ¡lisis completo en el Notebook (.ipynb) Â»](./notebooks/CNCEE_NLP_clean.ipynb)
