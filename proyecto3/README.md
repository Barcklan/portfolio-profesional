# ğŸ” Proyecto 3 â€” Interpretabilidad de Modelos Predictivos usando LIME y SHAP

> **Tema:** Explicabilidad y Ã©tica en inteligencia artificial

---

## ğŸ¯ Objetivo
Aplicar herramientas de **explicabilidad de modelos**, como **LIME** y **SHAP**, para analizar y justificar el comportamiento de un modelo de clasificaciÃ³n, destacando la importancia de la **transparencia y Ã©tica** en IA.

---

## ğŸ§© Contexto
Formar parte de un equipo que usa **IA en decisiones crÃ­ticas** implica construir modelos **explicables y auditables**.  
Este proyecto muestra cÃ³mo la interpretabilidad permite **detectar errores y sesgos ocultos** y mejorar la confianza en las decisiones automatizadas.

---

## ğŸ“Š Resumen del Proyecto
Se analizÃ³ un modelo de **Random Forest** para predecir **enfermedades cardÃ­acas**, utilizando **LIME** y **SHAP**:

- PrecisiÃ³n general: **88.6%**  
- Uso correcto de variables relevantes: **ECG, angina, frecuencia cardÃ­aca mÃ¡xima**  
- Problema detectado: valores de **colesterol mal interpretados** (`0.0`), causando falsos positivos

ğŸ“ˆ El proyecto demuestra que **precisiÃ³n sin interpretabilidad** no es suficiente, y que la transparencia es esencial en IA crÃ­tica.

---

## ğŸ§° TecnologÃ­as Utilizadas
- Python  
- Scikit-learn  
- Random Forest  
- LIME  
- SHAP  
- Matplotlib / Seaborn  
- Pandas / NumPy

---

## ğŸ“‚ Estructura de Archivos

```bash
 proyecto3/               
   â”œâ”€ README.md              â† Carpeta del Proyecto 3
   â”œâ”€ data/                 
   â”œâ”€ notebooks/             
   â”œâ”€ scripts/               
   â”œâ”€ reports/              
   â””â”€ requirements.txt       
  ```
