# ğŸ” Proyecto 3 â€” Interpretabilidad de Modelos Predictivos usando LIME y SHAP

> **EvaluaciÃ³n Modular - MÃ³dulo 9**  
> **Tema:** Explicabilidad y Ã©tica en inteligencia artificial

---

## ğŸ¯ Objetivo
Aplicar herramientas de **explicabilidad de modelos**, como **LIME** y **SHAP**, para analizar y justificar el comportamiento de un modelo de clasificaciÃ³n, destacando la importancia de la **transparencia y Ã©tica** en IA.

---

## ğŸ§© Contexto
Formar parte de un equipo que usa **IA en decisiones crÃ­ticas** implica construir modelos **explicables y auditables**.  
Este proyecto muestra cÃ³mo la interpretabilidad permite **detectar errores y sesgos ocultos** y mejorar la confianza en las decisiones automatizadas.

---

## ğŸ“Š Resumen del Proyecto
Se analizÃ³ un modelo de **Random Forest** para predecir **enfermedades cardÃ­acas**, utilizando **LIME** y **SHAP**:

- PrecisiÃ³n general: **88.6%**  
- Uso correcto de variables relevantes: **ECG, angina, frecuencia cardÃ­aca mÃ¡xima**  
- Problema detectado: valores de **colesterol mal interpretados** (`0.0`), causando falsos positivos

ğŸ“ˆ El proyecto demuestra que **precisiÃ³n sin interpretabilidad** no es suficiente, y que la transparencia es esencial en IA crÃ­tica.

---

## ğŸ§° TecnologÃ­as Utilizadas
- Python  
- Scikit-learn  
- Random Forest  
- LIME  
- SHAP  
- Matplotlib / Seaborn  
- Pandas / NumPy

---

## ğŸ“‚ Estructura de Archivos
```bash
 proyecto1/                 â† Carpeta del Proyecto 1
 
   â”œâ”€ README.md              â† DocumentaciÃ³n detallada del proyecto 1
   â”œâ”€ data/                  â† (Opcional) datasets utilizados
   â”œâ”€ notebooks/             â† Notebooks de anÃ¡lisis y entrenamiento
   â”œâ”€ scripts/               â† Scripts Python (.py) de entrenamiento, preprocesamiento, etc.
   â”œâ”€ reports/               â† GrÃ¡ficos, visualizaciones, resultados
   â””â”€ requirements.txt       â† Dependencias del proyecto
  ```
