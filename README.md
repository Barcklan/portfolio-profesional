# ğŸ’¼ Portafolio Profesional

## ğŸ‘‹ Sobre mÃ­

<img src="https://github.com/barcklan.png" width="120" align="left" style="border-radius: 50%; margin-right: 20px;">

Soy un profesional apasionado por el **anÃ¡lisis de datos**, la **modelaciÃ³n estadÃ­stica** y el **desarrollo de soluciones basadas en Machine Learning** que contribuyan a la toma de decisiones informadas.

**Nombre:** Claudio AndrÃ©s DÃ­az Vargas  
**EspecializaciÃ³n:** Ingeniero en EstadÃ­stica, especializado en Machine Learning y Ciencia de Datos  
**Correo:** [cdiazv.ies@gmail.com](mailto:cdiazv.ies@gmail.com)  
**GitHub:** [https://github.com/barcklan](https://github.com/barcklan)

Tengo experiencia en **anÃ¡lisis exploratorio de datos**, **modelado estadÃ­stico**, **tÃ©cnicas de aprendizaje supervisado y no supervisado**, **optimizaciÃ³n de modelos**, **visualizaciÃ³n de datos** y **automatizaciÃ³n de procesos analÃ­ticos** con Python.

Mi objetivo es **aplicar mis conocimientos en estadÃ­stica y Machine Learning** para diseÃ±ar soluciones basadas en datos que apoyen la toma de decisiones y generen impacto positivo.

---

## ğŸš€ Proyectos Destacados

A continuaciÃ³n, se presentan tres de mis proyectos mÃ¡s relevantes, donde aplico conocimientos en anÃ¡lisis, programaciÃ³n y documentaciÃ³n tÃ©cnica.

---

## ğŸ§  PROYECTO 1 â€” Sistema Inteligente de *Scoring* Crediticio con Redes Neuronales Profundas (DNN)

### ğŸ¯ Objetivo

DiseÃ±ar, entrenar y evaluar un modelo de red neuronal profunda para predecir la probabilidad de impago de clientes bancarios, utilizando un conjunto de datos realista.  
El modelo debe ser explicable, eficiente y presentar resultados interpretables para su uso en contextos financieros.

### ğŸ§© Contexto

Las entidades financieras deben decidir si otorgan o no un crÃ©dito a un cliente.  
Esta decisiÃ³n, si se basa en modelos poco explicables, puede generar sesgos, exclusiones injustas o pÃ©rdidas econÃ³micas.  

Se busca construir un modelo moderno, basado en **redes neuronales profundas**, que sea a la vez **preciso y explicable**, permitiendo a las instituciones mejorar la calidad de sus decisiones crediticias.

### ğŸ“Š Resumen

Se desarrollÃ³ un sistema de **scoring crediticio** basado en **DNN** para predecir la probabilidad de impago de clientes bancarios.

Tras un **anÃ¡lisis exploratorio** que identificÃ³ diferencias entre buenos y malos pagadores (en **monto y duraciÃ³n del crÃ©dito**), se aplicÃ³ **SMOTE** para balancear clases y se entrenaron dos modelos:

- **DNN simple:** *accuracy* de **70,5%**, **AUC = 0,78**, mostrando buena generalizaciÃ³n y equilibrio entre clases.  
- **ResNet tabular:** *accuracy* de **64%**, **AUC = 0,64**, mejor detecciÃ³n de clientes *bad* pero mÃ¡s falsos positivos.

ğŸ“ˆ La **DNN simple** se posiciona como la opciÃ³n mÃ¡s confiable y efectiva, aunque puede mejorarse la predicciÃ³n de clientes solventes y la interpretabilidad.

### ğŸ§° TecnologÃ­as Utilizadas

- **Python**
- **TensorFlow / Keras**
- **Scikit-learn**
- **Pandas / NumPy**
- **Matplotlib / Seaborn**
- **SMOTE (imbalanced-learn)**

ğŸ”— [Ver proyecto completo](./proyecto1)

---

## ğŸ§¬ PROYECTO 2 â€” ClasificaciÃ³n de Notas ClÃ­nicas para DetecciÃ³n Temprana de Afecciones  
### Con enfoque Ã©tico y mitigaciÃ³n de sesgos

### ğŸ¯ Objetivo

Desarrollar un sistema de **procesamiento de lenguaje natural (NLP)** capaz de clasificar textos mÃ©dicos (notas clÃ­nicas, sÃ­ntomas, diagnÃ³sticos) segÃºn su **gravedad clÃ­nica (leve, moderado, severo)**.  
El modelo debe incluir **buenas prÃ¡cticas de preprocesamiento, evaluaciÃ³n y mitigaciÃ³n de sesgos lingÃ¼Ã­sticos y sociales**.

### ğŸ§© Contexto

Los registros mÃ©dicos en texto libre contienen informaciÃ³n valiosa para detectar la gravedad de una afecciÃ³n de forma temprana.  
Sin embargo, su interpretaciÃ³n manual requiere tiempo, experiencia y puede verse afectada por **sesgos humanos**.

El sistema desarrollado clasifica automÃ¡ticamente las notas clÃ­nicas segÃºn su **nivel de gravedad**, ayudando a profesionales de la salud a **priorizar pacientes** y mejorar la eficiencia hospitalaria.  
AdemÃ¡s, se realizÃ³ un anÃ¡lisis Ã©tico sobre los posibles sesgos y se aplicaron mÃ©todos de **interpretabilidad** para garantizar confianza en su aplicaciÃ³n.

### ğŸ“Š Resumen

Se compararon dos enfoques:

- **Naive Bayes con TF-IDF:** enfoque clÃ¡sico, eficiente y explicable.  
- **BERT en espaÃ±ol:** modelo contextualizado de Ãºltima generaciÃ³n.

Ambos lograron **mÃ©tricas perfectas en validaciÃ³n**, lo que evidenciÃ³ tanto la capacidad de separaciÃ³n de los datos como un **riesgo de sobreajuste**.  
Se aplicaron tÃ©cnicas de **LIME** para interpretabilidad y se evaluaron riesgos Ã©ticos y sesgos potenciales.

ğŸ“ˆ El proyecto demuestra cÃ³mo los modelos de NLP pueden aplicarse en contextos clÃ­nicos de forma **efectiva, transparente y responsable**.

### ğŸ§° TecnologÃ­as Utilizadas

- **Python**
- **Scikit-learn**
- **Transformers (Hugging Face)**
- **BERT Multilingual / BETO**
- **LIME**
- **NLTK / spaCy**
- **Pandas / NumPy**

ğŸ”— [Ver proyecto completo](./proyecto2)

---

## ğŸ§© PROYECTO 3 â€” Interpretabilidad de Modelos Predictivos usando LIME y SHAP

### ğŸ¯ Objetivo

Aplicar herramientas de **explicabilidad de modelos**, como **LIME** y **SHAP**, para analizar y justificar el comportamiento de un modelo de clasificaciÃ³n, destacando la importancia de la transparencia en la inteligencia artificial.

### ğŸ§© Contexto

Formar parte de un equipo que usa **IA en decisiones crÃ­ticas** implica construir modelos **explicables y Ã©ticamente responsables**.  
Este proyecto explora cÃ³mo la interpretabilidad permite **auditar la lÃ³gica interna** de los modelos y detectar errores o sesgos ocultos.

### ğŸ“Š Resumen

Se analizÃ³ un modelo de **Random Forest** para predecir **enfermedades cardÃ­acas**, utilizando **LIME** y **SHAP**.  
El modelo alcanzÃ³ **88.6% de precisiÃ³n**, pero las explicaciones revelaron **fallas crÃ­ticas**:

- Uso correcto de variables relevantes (**ECG**, **angina**, **frecuencia cardÃ­aca mÃ¡xima**).  
- **Manejo incorrecto del colesterol**: interpretÃ³ valores bajos como riesgosos y altos como protectores, debido a datos con valores `0.0`.

ğŸ“‰ Sin interpretabilidad, este error habrÃ­a pasado inadvertido, comprometiendo decisiones clÃ­nicas.  
El caso demuestra que **precisiÃ³n sin transparencia** no es suficiente: la interpretabilidad garantiza **auditorÃ­a, confianza y Ã©tica** en modelos de IA.

### ğŸ§° TecnologÃ­as Utilizadas

- **Python**
- **Scikit-learn**
- **Random Forest**
- **LIME**
- **SHAP**
- **Matplotlib / Seaborn**
- **Pandas / NumPy**

ğŸ”— [Ver proyecto completo](./proyecto3)

---

## ğŸ§­ OrganizaciÃ³n y Buenas PrÃ¡cticas

Este portafolio estÃ¡ organizado de manera clara y estructurada:

- NavegaciÃ³n sencilla entre secciones.  
- DocumentaciÃ³n tÃ©cnica y reflexiva.  
- RedacciÃ³n cuidada, ortografÃ­a revisada y estilo profesional.

---

## ğŸŒ Enlace al Portafolio

Puedes acceder a este portafolio directamente en GitHub:  
ğŸ”— [https://github.com/barcklan/portfolio-profesional](https://github.com/barcklan/portfolio-profesional)
